# Sequence models & Attention mechanism  

## Learning Objectives 
* Sequence models can be augmented using an attention mechanism. This algorithm will help your model understand where it should focus its attention given a sequence of inputs. This week, you will also learn about speech recognition and how to deal with audio data.  

### 1. Sequence to Sequence Model 
* use a encoder & a decoder  
![](./img/wk03_seq2seq.png)  
* image captioning would use a similar model  
![](./img/wk03_image_captioning.png)  

### 2. Picking the Most Likely Sentence 
* Conditional Language Model  
![](./img/wk03_conditional_model.png)  
* don't ramdomly sample the words for translation, choose the sentence that maximises the probability  
![](./img/wk03_most_likely_sentence.png)  
* why not greedy search? - not good enough (might not be optimal)
![](./img/wk03_greedy.png)  

### 3. Beam Search 
* Beam Width: the number of words/sequences considered in each step in terms of the joint probability of p(y1, y2, .. yn | X) where n = 1 .. Ty. 
![](./img/wk03_beam_search.png)  
![](./img/wk03_beam_search2.png)  
![](./img/wk03_beam_search3.png)  
_if the beam width == 1, then it will just be greedy search._  
* __log transformation__: the product of probabilities can be extremely small. 
* __length normalization__: make sure probability measurement doesn't prefer shorter sentences. 
![](./img/wk03_length_norm.png)  
![](./img/wk03_beam_performance.png)  

### 4. Error Analysis in Beam Search 
* which one to blame? RNN or Beam Search? 
![](./img/wk03_RNN_Beam.png)  
* analysis on Beam Search 
![](./img/wk03_analysis.png)  
![](./img/wk03_analysis2.png)  

### 5. _Bleu_ Score 
* bleu score: whether any of the words generated by the machine is in the human translation set and how many. 
* modified precision: has a clip on the number of valid counts is capped at the max number of appearance in one of the reference sentence. 
![](./img/wk03_eval_MT.png) 
* we would also like to look at pairs of words.  
![](./img/wk03_bigrams.png)  
![](./img/wk03_n_grams.png)  
* the final bleu score 
![](./img/wk03_bleu_score.png) 

### 6. Attention Model 
* the blue line is the basic seq to seq model while the green line is the attention model. 
![](./img/wk03_long_seq.png) 
* introducing attention weights alpha. This allows for a weighted window to be considered for each translation step:    
![](./img/wk03_attention_model.png)  
* 